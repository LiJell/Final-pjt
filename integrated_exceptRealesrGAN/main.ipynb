{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","mount_file_id":"1EXa9u5vB0hUStFQWiHEo_LsGTi73WjPJ","authorship_tag":"ABX9TyN5vz1B1hwE8tTHeDQ9dynE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# 무조건 Real-ESRGAN 파일들이 존재하는 곳에서 실행해야 문제없이 돌아감"],"metadata":{"id":"-JBvRMpv4Pok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd /content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/Real-ESRGAN"],"metadata":{"id":"Oym_8gv7qMZH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Layer, InputSpec\n","import PIL\n","import cv2\n","import matplotlib.pyplot as plt\n","import torch\n","from torchvision import transforms, models"],"metadata":{"id":"vOkaaCtUqPNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# real esrGAN 환경 설정\n","print('환경을 설정 중입니다')\n","!pip install basicsr\n","!pip install facexlib\n","!pip install gfpgan\n","!pip install -r requirements.txt\n","!python setup.py develop\n","print('환경 설정 완료')"],"metadata":{"id":"wdsyihEBqRDi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_path = '/content/drive/MyDrive/LimnPet_project/dog_7928.jpg' # 인풋이미지 경로\n","style = 'others' # 원하는 스타일명 지정\n","    # STYLES = [\"shinkai\", \"hayao\", \"hosoda\", \"paprika\"] or ['Hayao'] others(직접학습시킨 경우, 각각 이름 지정해주고 코드수정하면됨)\n","\n","print('강아지 영역 추출 시작')\n","# segmentation\n","model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n","\n","cmap = plt.cm.get_cmap('tab20c')\n","colors = (cmap(np.arange(cmap.N)) * 255).astype(int)[:, :3].tolist()\n","np.random.seed(2020)\n","colors.insert(0, [0, 0, 0]) \n","colors = np.array(colors, dtype=np.uint8)\n","\n","\n","# segmentation 함수 정의\n","def segment(net, img):\n","    preprocess = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        ),\n","    ])\n","\n","    input_tensor = preprocess(img)\n","    input_batch = input_tensor.unsqueeze(0)\n","\n","    if torch.cuda.is_available():\n","        input_batch = input_batch.to('cuda')\n","        model.to('cuda')\n","\n","    output = model(input_batch)['out'][0] # (21, height, width)\n","\n","    output_predictions = output.argmax(0).byte().cpu().numpy() # (height, width) \n","\n","    r = PIL.Image.fromarray(output_predictions).resize((img.shape[1], img.shape[0]))\n","    r.putpalette(colors)\n","\n","    return r, output_predictions\n","\n","img = np.array(PIL.Image.open(image_path))\n","fg_h, fg_w, _ = img.shape\n","segment_map, pred = segment(model, img)\n","background = np.ones((fg_h, fg_w, 3))*255.0\n","mask = (pred == 12).astype(float) * 255 # 12: dog\n","_, alpha = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY)\n","alpha = cv2.GaussianBlur(alpha, (7, 7), 0).astype(float)\n","alpha_scale = alpha / 255. # (height, width)\n","alpha_1 = np.repeat(np.expand_dims(alpha_scale, axis=2), 3, axis=2) # (height, width, 3)\n","foreground = cv2.multiply(alpha_1, img.astype(float))\n","background = cv2.multiply(1. - alpha_1, background.astype(float))  \n","result = cv2.add(foreground, background).astype(np.uint8)\n","print('강아지 영역 추출 완료')\n","\n","print('그림그리기 시작')\n","# cartonGAN\n","# pretrained 모델의 load에 필요한 부분\n","class InstanceNormalization(Layer):\n","    def __init__(self,\n","                 axis=3,\n","                 epsilon=1e-9,\n","                 center=True,\n","                 scale=True,\n","                 beta_initializer='zeros',\n","                 gamma_initializer='ones',\n","                 beta_regularizer=None,\n","                 gamma_regularizer=None,\n","                 beta_constraint=None,\n","                 gamma_constraint=None,\n","                 **kwargs):\n","        super(InstanceNormalization, self).__init__(**kwargs)\n","        self.supports_masking = True\n","        self.axis = axis\n","        self.epsilon = epsilon\n","        self.center = center\n","        self.scale = scale\n","        self.beta_initializer = tf.keras.initializers.get(beta_initializer)\n","        self.gamma_initializer = tf.keras.initializers.get(gamma_initializer)\n","        self.beta_regularizer = tf.keras.regularizers.get(beta_regularizer)\n","        self.gamma_regularizer = tf.keras.regularizers.get(gamma_regularizer)\n","        self.beta_constraint = tf.keras.constraints.get(beta_constraint)\n","        self.gamma_constraint = tf.keras.constraints.get(gamma_constraint)\n","\n","    def build(self, input_shape):\n","        ndim = len(input_shape)\n","        if self.axis == 0:\n","            raise ValueError('Axis cannot be zero')\n","\n","        if (self.axis is not None) and (ndim == 2):\n","            raise ValueError('Cannot specify axis for rank 1 tensor')\n","\n","        self.input_spec = tf.keras.layers.InputSpec(ndim=ndim)\n","\n","        if self.axis is None:\n","            shape = (1,)\n","        else:\n","            shape = (input_shape[self.axis],)\n","\n","        if self.scale:\n","            self.gamma = self.add_weight(shape=shape,\n","                                         name='gamma',\n","                                         initializer=self.gamma_initializer,\n","                                         regularizer=self.gamma_regularizer,\n","                                         constraint=self.gamma_constraint)\n","        else:\n","            self.gamma = None\n","        if self.center:\n","            self.beta = self.add_weight(shape=shape,\n","                                        name='beta',\n","                                        initializer=self.beta_initializer,\n","                                        regularizer=self.beta_regularizer,\n","                                        constraint=self.beta_constraint)\n","        else:\n","            self.beta = None\n","        self.built = True\n","\n","    def call(self, inputs, training=None):\n","        input_shape = tf.keras.backend.int_shape(inputs)\n","        reduction_axes = list(range(0, len(input_shape)))\n","\n","        if self.axis is not None:\n","            del reduction_axes[self.axis]\n","\n","        del reduction_axes[0]\n","\n","        mean = tf.keras.backend.mean(inputs, reduction_axes, keepdims=True)\n","        stddev = tf.keras.backend.std(inputs, reduction_axes, keepdims=True) + self.epsilon\n","        normed = (inputs - mean) / stddev\n","\n","        broadcast_shape = [1] * len(input_shape)\n","        if self.axis is not None:\n","            broadcast_shape[self.axis] = input_shape[self.axis]\n","\n","        if self.scale:\n","            broadcast_gamma = tf.keras.backend.reshape(self.gamma, broadcast_shape)\n","            normed = normed * broadcast_gamma\n","        if self.center:\n","            broadcast_beta = tf.keras.backend.reshape(self.beta, broadcast_shape)\n","            normed = normed + broadcast_beta\n","        return normed\n","\n","    def get_config(self):\n","        config = {\n","            'axis': self.axis,\n","            'epsilon': self.epsilon,\n","            'center': self.center,\n","            'scale': self.scale,\n","            'beta_initializer': tf.keras.initializers.serialize(self.beta_initializer),\n","            'gamma_initializer': tf.keras.initializers.serialize(self.gamma_initializer),\n","            'beta_regularizer': tf.keras.regularizers.serialize(self.beta_regularizer),\n","            'gamma_regularizer': tf.keras.regularizers.serialize(self.gamma_regularizer),\n","            'beta_constraint': tf.keras.constraints.serialize(self.beta_constraint),\n","            'gamma_constraint': tf.keras.constraints.serialize(self.gamma_constraint)\n","        }\n","        base_config = super(InstanceNormalization, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class ReflectionPadding2D(Layer):\n","    def __init__(self, padding=(1, 1), **kwargs):\n","        self.padding = tuple(padding)\n","        self.input_spec = [InputSpec(ndim=4)]\n","        super(ReflectionPadding2D, self).__init__(**kwargs)\n","\n","    def compute_output_shape(self, s):\n","        \"\"\" If you are using \"channels_last\" configuration\"\"\"\n","        return s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3]\n","\n","    def call(self, x):\n","        w_pad, h_pad = self.padding\n","        return tf.pad(x, [[0, 0], [h_pad, h_pad], [w_pad, w_pad], [0, 0]], 'REFLECT')\n","\n","\n","def conv_layer(style, name, filters, kernel_size, strides=(1, 1), bias=True):\n","    init_weight = np.load(f\"{PRETRAINED_WEIGHT_DIR}/{style}/{name}.weight.npy\")\n","    init_weight = np.transpose(init_weight, [2, 3, 1, 0])\n","    init_bias = np.load(f\"{PRETRAINED_WEIGHT_DIR}/{style}/{name}.bias.npy\")\n","\n","    if bias:\n","        bias_initializer = tf.keras.initializers.constant(init_bias)\n","    else:\n","        bias_initializer = \"zeros\"\n","\n","    layer = tf.keras.layers.Conv2D(\n","        filters=filters,\n","        kernel_size=kernel_size,\n","        strides=strides,\n","        kernel_initializer=tf.keras.initializers.constant(init_weight),\n","        bias_initializer=bias_initializer\n","    )\n","    return layer\n","\n","\n","def instance_norm_layer(style, name, epsilon=1e-9):\n","    init_beta = np.load(f\"{PRETRAINED_WEIGHT_DIR}/{style}/{name}.shift.npy\")\n","    init_gamma = np.load(f\"{PRETRAINED_WEIGHT_DIR}/{style}/{name}.scale.npy\")\n","\n","    layer = InstanceNormalization(\n","        axis=-1,\n","        epsilon=epsilon,\n","        beta_initializer=tf.keras.initializers.Constant(init_beta),\n","        gamma_initializer=tf.keras.initializers.Constant(init_gamma)\n","    )\n","    return layer\n","\n","\n","def deconv_layers(style, name, filters, kernel_size, strides=(1, 1)):\n","    init_weight = np.load(f\"{PRETRAINED_WEIGHT_DIR}/{style}/{name}.weight.npy\")\n","    init_weight = np.transpose(init_weight, [2, 3, 1, 0])\n","    init_bias = np.load(f\"{PRETRAINED_WEIGHT_DIR}/{style}/{name}.bias.npy\")\n","\n","    layers = list()\n","    layers.append(tf.keras.layers.Conv2DTranspose(\n","        filters=filters,\n","        kernel_size=kernel_size,\n","        strides=strides,\n","        kernel_initializer=tf.keras.initializers.constant(init_weight),\n","        bias_initializer=tf.keras.initializers.constant(init_bias)\n","    ))\n","\n","    layers.append(tf.keras.layers.Cropping2D(cropping=((1, 0), (1, 0))))\n","    return layers\n","\n","\n","def load_model(style):\n","    inputs = tf.keras.Input(shape=(None, None, 3))\n","\n","    y = ReflectionPadding2D(padding=(3, 3))(inputs)\n","    y = conv_layer(style, \"conv01_1\", filters=64, kernel_size=7)(y)\n","    y = instance_norm_layer(style, \"in01_1\")(y)\n","    y = tf.keras.layers.Activation(\"relu\")(y)\n","\n","    y = ReflectionPadding2D(padding=(1, 1))(y)\n","    y = conv_layer(style, \"conv02_1\", filters=128, kernel_size=3, strides=(2, 2))(y)\n","    y = ReflectionPadding2D(padding=(1, 1))(y)\n","    y = conv_layer(style, \"conv02_2\", filters=128, kernel_size=3, strides=(1, 1))(y)\n","    y = instance_norm_layer(style, \"in02_1\")(y)\n","    y = tf.keras.layers.Activation(\"relu\")(y)\n","\n","    y = ReflectionPadding2D(padding=(1, 1))(y)\n","    y = conv_layer(style, \"conv03_1\", filters=256, kernel_size=3, strides=(2, 2))(y)\n","    y = ReflectionPadding2D(padding=(1, 1))(y)\n","    y = conv_layer(style, \"conv03_2\", filters=256, kernel_size=3, strides=(1, 1))(y)\n","    y = instance_norm_layer(style, \"in03_1\")(y)\n","\n","    t_prev = tf.keras.layers.Activation(\"relu\")(y)\n","\n","    for i in range(4, 12):\n","        y = ReflectionPadding2D(padding=(1, 1))(t_prev)\n","        y = conv_layer(style, \"conv%02d_1\" % i, filters=256, kernel_size=3)(y)\n","        y = instance_norm_layer(style, \"in%02d_1\" % i)(y)\n","        y = tf.keras.layers.Activation(\"relu\")(y)\n","\n","        t = ReflectionPadding2D(padding=(1, 1))(y)\n","        t = conv_layer(style, \"conv%02d_2\" % i, filters=256, kernel_size=3)(t)\n","        t = instance_norm_layer(style, \"in%02d_2\" % i)(t)\n","\n","        t_prev = tf.keras.layers.Add()([t, t_prev])\n","\n","        if i == 11:\n","            y = t_prev\n","\n","    layers = deconv_layers(style, \"deconv01_1\", filters=128, kernel_size=3, strides=(2, 2))\n","    for layer in layers:\n","        y = layer(y)\n","    y = ReflectionPadding2D(padding=(1, 1))(y)\n","    y = conv_layer(style, \"deconv01_2\", filters=128, kernel_size=3)(y)\n","    y = instance_norm_layer(style, \"in12_1\")(y)\n","    y = tf.keras.layers.Activation(\"relu\")(y)\n","\n","    layers = deconv_layers(style, \"deconv02_1\", filters=64, kernel_size=3, strides=(2, 2))\n","    for layer in layers:\n","        y = layer(y)\n","    y = ReflectionPadding2D(padding=(1, 1))(y)\n","    y = conv_layer(style, \"deconv02_2\", filters=64, kernel_size=3)(y)\n","    y = instance_norm_layer(style, \"in13_1\")(y)\n","    y = tf.keras.layers.Activation(\"relu\")(y)\n","\n","    y = ReflectionPadding2D(padding=(3, 3))(y)\n","    y = conv_layer(style, \"deconv03_1\", filters=3, kernel_size=7)(y)\n","    y = tf.keras.layers.Activation(\"tanh\")(y)\n","\n","    model = tf.keras.Model(inputs=inputs, outputs=y)\n","\n","    return model\n","  \n","# pretrained 모델을 이용할 경우 아래 실행\n","if (style == \"shinkai\") | (style == \"hayao\") | (style == \"hosoda\") | (style == \"paprika\"):\n","    PRETRAINED_WEIGHT_DIR = '/content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/cartoongan/pretrained_weights' # 저장된 pretrained 모델 가중치 파일 경로\n","    cartoonGAN_model = load_model(style)\n","    input_image = result\n","    input_image = np.expand_dims(input_image, axis=0)\n","    output_image = cartoonGAN_model.predict(input_image)\n","    output_image = output_image[0]\n","    output_image = output_image[:,:,[2,1,0]]\n","    output_image = output_image * 0.5 + 0.5\n","    print('그림그리기 완료')\n","    print('배경 투명화 시작')\n","    alpha_output = cv2.cvtColor(output_image, cv2.COLOR_RGB2RGBA)\n","    resize_alpha = cv2.resize(alpha_scale, (output_image.shape[1],output_image.shape[0]))\n","    alpha_output[:,:,3] = resize_alpha\n","    alpha_output_2= alpha_output*255\n","    alpha_output_2 = alpha_output_2.astype(np.uint8)\n","    alpha_output_2 = cv2.cvtColor(alpha_output_2, cv2.COLOR_RGBA2BGRA)\n","    cv2.imwrite('/content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/Real-ESRGAN/upload/output_image.png', alpha_output_2)\n","    print('배경 투명화 완료')\n","\n","elif style=='Hayao' : \n","    model_dir = '/content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/saved_model/Hayao' # 저장한 모델 디렉토리 경로\n","    model = tf.saved_model.load(model_dir)\n","    f = model.signatures[\"serving_default\"]\n","    input_image = result\n","    input_image = np.expand_dims(input_image, 0).astype(np.float32) / 127.5 -1\n","    output_image = f(tf.constant(input_image))['output_1']\n","    output_image = ((output_image.numpy().squeeze() + 1) * 127.5).astype(np.uint8)\n","    print('그림그리기 완료')\n","    print('배경 투명화 시작')\n","    alpha_output = cv2.cvtColor(output_image, cv2.COLOR_RGB2RGBA)\n","    resize_alpha = cv2.resize(alpha, (output_image.shape[1],output_image.shape[0]))\n","    alpha_output[:,:,3] = resize_alpha\n","    alpha_output_2= alpha_output\n","    alpha_output_2 = alpha_output_2.astype(np.uint8)\n","    alpha_output_2 = cv2.cvtColor(alpha_output_2, cv2.COLOR_RGBA2BGRA)\n","    cv2.imwrite('/content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/Real-ESRGAN/upload/output_image.png', alpha_output_2)\n","    print('배경 투명화 완료')\n","\n","\n","# 직접 학습한 모델 이용할 경우 아래 실행(모델 수나 경로에 맞춰 수정 필요)\n","else : \n","    model_dir = '/content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/saved_model/SavedModel_0000' # 저장한 모델 디렉토리 경로\n","    model = tf.saved_model.load(model_dir)\n","    f = model.signatures[\"serving_default\"]\n","    input_image = result\n","    input_image = np.expand_dims(input_image, 0).astype(np.float32) / 127.5 -1\n","    output_image = f(tf.constant(input_image))['output_1']\n","    output_image = ((output_image.numpy().squeeze() + 1) * 127.5).astype(np.uint8)\n","    print('그림그리기 완료')\n","    print('배경 투명화 시작')\n","    alpha_output = cv2.cvtColor(output_image, cv2.COLOR_RGB2RGBA)\n","    resize_alpha = cv2.resize(alpha, (output_image.shape[1],output_image.shape[0]))\n","    alpha_output[:,:,3] = resize_alpha\n","    alpha_output_2= alpha_output\n","    alpha_output_2 = alpha_output_2.astype(np.uint8)\n","    alpha_output_2 = cv2.cvtColor(alpha_output_2, cv2.COLOR_RGBA2BGRA)\n","    cv2.imwrite('/content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/Real-ESRGAN/upload/output_image.png', alpha_output_2)\n","    print('배경 투명화 완료')\n","\n","\n","print('해상도 높이기 시작')\n","\n","!python inference_realesrgan.py -n RealESRGAN_x4plus_anime_6B \\\n","                                -i /content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/Real-ESRGAN/upload/output_image.png \\\n","                                -o /content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/Real-ESRGAN/results\n","print('해상도 높이기 완료')\n","\n","last_output = cv2.imread('/content/drive/MyDrive/multi_final_project/integrated_exceptRealesrGAN/Real-ESRGAN/upload/output_image.png', cv2.IMREAD_UNCHANGED)\n","last_output = cv2.cvtColor(last_output, cv2.COLOR_BGRA2RGBA)\n","\n","print('모든 결과 확인')\n","plt.figure(figsize=(20,15))\n","ax = plt.subplot(1,5,1)\n","ax.imshow(img)\n","ax.axis('off')\n","ax.set_title('raw')\n","ax = plt.subplot(1,5,2)\n","ax.imshow(result)\n","ax.axis('off')\n","ax.set_title('segmentation')\n","ax = plt.subplot(1,5,3)\n","ax.imshow(output_image)\n","ax.axis('off')\n","ax.set_title('cartonnGAN')\n","ax = plt.subplot(1,5,4)\n","ax.imshow(alpha_output)\n","ax.axis('off')\n","ax.set_title('alpha')\n","ax = plt.subplot(1,5,5)\n","ax.imshow(last_output)\n","ax.axis('off')\n","ax.set_title('real_esrGAN')"],"metadata":{"id":"Wl-vJZwSqYay"},"execution_count":null,"outputs":[]}]}